{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2SpaceMasterRace/2SpaceMasterRace/blob/main/Transformer_MappingNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Packages"
      ],
      "metadata": {
        "id": "CXfGyGscQNMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-wOHs80SXDx",
        "outputId": "ef378cea-2291-4af8-a93d-5ff39ef73057"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "git version 2.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l46UbkMaOzY5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b424d30f-7f73-46a9-838c-9a8f3836a5b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu117, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.14.1+cu116)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.9/dist-packages (0.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-4zuy5uu0\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-4zuy5uu0\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a9b1bf5920416aaeaec965c25dd9e8f98c864f16\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (4.65.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (0.14.1+cu116)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.9/dist-packages (from ftfy->clip==1.0) (0.2.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision->clip==1.0) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->clip==1.0) (8.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision->clip==1.0) (1.22.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (2022.12.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117\n",
        "!pip install transformers\n",
        "!pip install numpy\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Libraries"
      ],
      "metadata": {
        "id": "rgcZUFDx8Dxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "import os\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as nnf\n",
        "import sys\n",
        "from typing import Tuple, List, Union, Optional\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm, trange\n",
        "from google.colab import files\n",
        "from enum import Enum"
      ],
      "metadata": {
        "id": "kEzbnK5F8Bau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLIP Model"
      ],
      "metadata": {
        "id": "d5dmv-El6zBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "T = torch.Tensor\n",
        "\n",
        "\n",
        "D = torch.device\n",
        "CPU = torch.device('cpu')\n",
        "\n",
        "\n",
        "def get_device(device_id: int) -> D:\n",
        "    if not torch.cuda.is_available():\n",
        "        return CPU\n",
        "    device_id = min(torch.cuda.device_count() - 1, device_id)\n",
        "    return torch.device(f'cuda:{device_id}')\n",
        "\n",
        "\n",
        "CUDA = get_device\n",
        "\n",
        "current_directory = os.getcwd()\n",
        "save_path = os.path.join(os.path.dirname(current_directory), \"pretrained_models\")\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "model_path = os.path.join(save_path, 'model_wieghts.pt')\n",
        "\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def forward(self, x: T) -> T: #takes input tensor x\n",
        "        return self.model(x)\n",
        "\n",
        "    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh): #defining MLP with size of nn, bias and activation\n",
        "        super(MLP, self).__init__()\n",
        "        layers = []\n",
        "        for i in range(len(sizes) -1):\n",
        "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias)) #if bias is true new layer is added\n",
        "            if i < len(sizes) - 2:\n",
        "                layers.append(act())\n",
        "        self.model = nn.Sequential(*layers) #displays output sequentially\n",
        "\n",
        "\n",
        "class ClipCaptionModel(nn.Module):\n",
        "\n",
        "    #@functools.lru_cache #FIXME\n",
        "    def get_dummy_token(self, batch_size: int, device: D) -> T: #creates a tensor of details from batch size and device\n",
        "        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n",
        "\n",
        "    def forward(self, tokens: T, prefix: T, mask: Optional[T] = None, labels: Optional[T] = None):\n",
        "        embedding_text = self.gpt.transformer.wte(tokens) # creates a tensor of word embeddings, key words in a numerical form\n",
        "        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size) #new tensor of prefix\n",
        "        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1) #concatenates both the sensors and crates a new tensor \n",
        "        if labels is not None: #in case of pre- existing labels\n",
        "            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n",
        "            labels = torch.cat((dummy_token, tokens), dim=1)\n",
        "        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask) #output tensor formed \n",
        "        return out\n",
        "\n",
        "    def __init__(self, prefix_length: int, prefix_size: int = 512):\n",
        "        super(ClipCaptionModel, self).__init__()\n",
        "        self.prefix_length = prefix_length\n",
        "        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
        "        if prefix_length > 10:  #if greater than 10 a linear projection layer is used otherwise a MLP with two hidden layers is used \n",
        "            self.clip_project = nn.Linear(prefix_size, self.gpt_embedding_size * prefix_length)\n",
        "        else:\n",
        "            self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2, self.gpt_embedding_size * prefix_length))\n",
        "\n",
        "\n",
        "class ClipCaptionPrefix(ClipCaptionModel):\n",
        "\n",
        "    def parameters(self, recurse: bool = True): #parameters of projection layer\n",
        "        return self.clip_project.parameters()\n",
        "\n",
        "    def train(self, mode: bool = True): #sets to evaluation mode so data is not updated during training\n",
        "        super(ClipCaptionPrefix, self).train(mode)\n",
        "        self.gpt.eval()\n",
        "        return self"
      ],
      "metadata": {
        "id": "VZx30OFw6urc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mapping Network"
      ],
      "metadata": {
        "id": "PiaZnKauPwnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MlpTransformer(nn.Module):\n",
        "     def __init__(self, in_dim, h_dim, out_d: Optional[int] = None, act=nnf.relu, dropout=0.): #Input dimension, Hidden dimension, output dimension, RELU activation function\n",
        "         super().__init__()\n",
        "         out_d = out_d if out_d is not None else in_dim #Output dimension of nn\n",
        "         self.fc1 = nn.Linear(in_dim, h_dim) #First Fully Connected Layer \n",
        "         self.act = act #Activation function at first FC\n",
        "         self.fc2 = nn.Linear(h_dim, out_d) #Second Fully Connected Layer \n",
        "         self.dropout = nn.Dropout(dropout) #Dropout layer at 2nd FC i.e,drops random inputs or sets it to zero to prevent overfitting \n",
        "\n",
        "     def forward(self, x): # x is the input\n",
        "         x = self.fc1(x) #applying linear transformation using weights and bias on x and storing back on x\n",
        "         x = self.act(x)\n",
        "         x = self.dropout(x) #sets random inputs of x to 0\n",
        "         x = self.fc2(x) #linear transformation on output of dropout layer\n",
        "         x = self.dropout(x)\n",
        "         return x\n",
        "\n",
        "class MLP(nn.Module): #building a MLP to compare with the transformer\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor: #input x of type tensor, returns a tensor output\n",
        "        return self.model(x)\n",
        "\n",
        "    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh): #size of input and output layer, bias and activation function\n",
        "        super(MLP, self).__init__()\n",
        "        layers = []\n",
        "        for i in range(len(sizes) - 1): #loop from 0 to size -1\n",
        "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n",
        "            if i < len(sizes) - 2:\n",
        "                layers.append(act())\n",
        "        self.model = nn.Sequential(*layers) #sequentially shows the output\n",
        "\n",
        "\n",
        "'''\n",
        "Forward mechanism in multi-headed attention layer:\n",
        "1.The input is transformed to queries and keys using a linear layer\n",
        "2.separate the keys and values tensors along the third dimension (adding 2 in index 2 of the parameters)\n",
        "3.compute attention score\n",
        "4.check if masking is applied\n",
        "5.apply softmax\n",
        "6.compute weighted sum and reshape + apply linear transformation\n",
        "'''\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "''' Arguments:\n",
        "dim_self  : an integer representing the input dimensionality of the queries, keys, and values.\n",
        "din_ref   : an integer representing the input dimensionality of the reference tensor.\n",
        "num_heads : an integer representing the number of attention heads to use.\n",
        "bias      : a boolean indicating whether or not to include bias terms in the linear layers.\n",
        "dropout: a float representing the dropout probability.   '''\n",
        "                                                                          \n",
        "    def __init__(self, dim_self, dim_ref, num_heads, bias=True, dropout=0.): #dimensions of nn as well ass head i.e., headers are defined\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim_self // num_heads\n",
        "        self.scale = head_dim ** -0.5 #calculates the scaling factor for the attention scores by taking the inverse score root of head_dim\n",
        "        self.to_queries = nn.Linear(dim_self, dim_self, bias=bias) #mapping to query vectors\n",
        "        self.to_keys_values = nn.Linear(dim_ref, dim_self * 2, bias=bias) #maps the input to the key and value vectors. Input size dim_ref, output size dim_self*2\n",
        "        self.project = nn.Linear(dim_self, dim_self) #maps concatenated output of query, key and value vectors to final outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, y=None, mask=None):# x is input tensor and y is another tensor for computing keys and values and mask tensor\n",
        "        y = y if y is not None else x #y=x if y is none\n",
        "        b, n, c = x.shape #b=batch size, n=sequence length and c=input dimensionality\n",
        "        _, m, d = y.shape #m=reference sequence length,d=reference input dimensionality\n",
        "        # b n h dh\n",
        "        queries = self.to_queries(x).reshape(b, n, self.num_heads, c // self.num_heads) #output is reshaped to have the above dimensions\n",
        "        # b m 2 h dh\n",
        "        keys_values = self.to_keys_values(y).reshape(b, m, 2, self.num_heads, c // self.num_heads)\n",
        "        keys, values = keys_values[:, :, 0], keys_values[:, :, 1] #extracts the key and value vectors from the reshaped keys_values tensor\n",
        "        attention = torch.einsum('bnhd,bmhd->bnmh', queries, keys) * self.scale ''' torch.einsun computes the attention scores by taking the dot product between the queries and keys\n",
        "                                                                                  then scale the result by the square root of the head dimension (`self.scale`) - Batch Multiplication\n",
        "                                                                                '''                                                                                                   \n",
        "        if mask is not None:\n",
        "            if mask.dim() == 2: #checks if dim of mask is 2\n",
        "                mask = mask.unsqueeze(1)  ''' adds a singleton dimension at index 1 to match the shape of the attention tensor '''\n",
        "            attention = attention.masked_fill(mask.unsqueeze(3), float(\"-inf\")) #applying mask to attention tensor\n",
        "        attention = attention.softmax(dim=2)\n",
        "        out = torch.einsum('bnmh,bmhd->bnhd', attention, values).reshape(b, n, c) '''computes weighted sum of the values using computed attention weights and reshapes the output to specified dim'''\n",
        "        out = self.project(out)\n",
        "        return out, attention\n",
        "\n",
        "\n",
        "class TransformerLayer(nn.Module):\n",
        "\n",
        "    def forward_with_attention(self, x, y=None, mask=None): \n",
        "        x_, attention = self.attn(self.norm1(x), y, mask) #output is the tensor x and attention tensor\n",
        "        x = x + x_ #input x plus transformed x tensor\n",
        "        x = x + self.mlp(self.norm2(x)) #applies mlptransformer to x and adds the result to x \n",
        "        return x, attention #returns 2 tensors\n",
        "\n",
        "    def forward(self, x, y=None, mask=None):\n",
        "        x = x + self.attn(self.norm1(x), y, mask)[0] #applies multihead attention layer to input x and y uding mask. The output is transformed x added to x\n",
        "        x = x + self.mlp(self.norm2(x)) \n",
        "        return x #returns a single tensor\n",
        "\n",
        "    def __init__(self, dim_self, dim_ref, num_heads, mlp_ratio=4., bias=False, dropout=0., act=nnf.relu, #mlp ratio=ratio of hidden dimension\n",
        "                 norm_layer: nn.Module = nn.LayerNorm): # norm_layer=normalisation layer\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim_self)\n",
        "        self.attn = MultiHeadAttention(dim_self, dim_ref, num_heads, bias=bias, dropout=dropout)\n",
        "        self.norm2 = norm_layer(dim_self)\n",
        "        self.mlp = MlpTransformer(dim_self, int(dim_self * mlp_ratio), act=act, dropout=dropout)\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "    def forward_with_attention(self, x, y=None, mask=None):\n",
        "        attentions = []\n",
        "        for layer in self.layers:\n",
        "            x, att = layer.forward_with_attention(x, y, mask)\n",
        "            attentions.append(att)\n",
        "        return x, attentions\n",
        "\n",
        "    def forward(self, x, y=None, mask=None):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if i % 2 == 0 and self.enc_dec: # cross\n",
        "                x = layer(x, y)\n",
        "            elif self.enc_dec:  # self\n",
        "                x = layer(x, x, mask)\n",
        "            else:  # self or cross\n",
        "                x = layer(x, y, mask)\n",
        "        return x\n",
        "\n",
        "    def __init__(self, dim_self: int, num_heads: int, num_layers: int, dim_ref: Optional[int] = None,\n",
        "                 mlp_ratio: float = 2., act=nnf.relu, norm_layer: nn.Module = nn.LayerNorm, enc_dec: bool = False):\n",
        "        super(Transformer, self).__init__()\n",
        "        dim_ref = dim_ref if dim_ref is not None else dim_self  #sets dimension of the reference embedding to the input embedding dimension if dim_ref is not provided\n",
        "        #in some cases reference embedding = input embedding\n",
        "        self.enc_dec = enc_dec  #checks whether transformer is an encoder decoder architecture or not\n",
        "        if enc_dec:\n",
        "            num_layers = num_layers * 2   #if it is, number of layers is doubled to account for the encoder and decoder\n",
        "        layers = [] #create layer of the transformer\n",
        "        for i in range(num_layers):\n",
        "            if i % 2 == 0 and enc_dec:  # cross\n",
        "                layers.append(TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n",
        "            elif enc_dec:  # self\n",
        "                layers.append(TransformerLayer(dim_self, dim_self, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n",
        "            else:  # self or cross\n",
        "                layers.append(TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n",
        "       #if enc_dec = true and the iteration number is even, a cross attention layer is created that takes the input and ref embedding as inputs\n",
        "       #if enc_dec = true and the iteration number is odd, a self attention layer is created that takes the output of previous layer as input\n",
        "       #if enc_dec = false a self attention layer is created that takes the input embedding as input\n",
        "        self.layers = nn.ModuleList(layers) #allows transformer layers to be treated as a single module.\n",
        "\n",
        "\n",
        "\n",
        "class TransformerMapper(nn.Module):\n",
        "\n",
        "       def forward(self, x): #input x is tensor\n",
        "        x = self.linear(x).view(x.shape[0], self.clip_length, -1) #x is passed through linear layer that maps the input tensor dim_clip to clip_length * dimembedding\n",
        "        #the resulting tensor is then reshaped. This converts the flat representation of the input into a 3D tensor where-\n",
        "        #first dimension represents batch size\n",
        "        #second dimension represents the length of the input sequence\n",
        "        #third dimension represents the size of the embedding space\n",
        "        prefix = self.prefix_const.unsqueeze(0).expand(x.shape[0], *self.prefix_const.shape)  #prefix tensor is created by duplicating along the batch dimension and then expanding the tensor along the batch dimension to match the batch size of the input tensor\n",
        "        prefix = torch.cat((x, prefix), dim=1)  #The prefix tensor is then concatenated with the input tensor along the second dimension\n",
        "        #this adds the prefix to the beginning of each input seq\n",
        "        out = self.transformer(prefix)[:, self.clip_length:]  #concatenated tensor is passed through the transformer model\n",
        "        return out\n",
        "\n",
        "    def __init__(self, dim_clip: int, dim_embedding: int, prefix_length: int, clip_length: int, num_layers: int = 8):\n",
        "        super(TransformerMapper, self).__init__()   #this line class the constructor of the supercalss of transformermapper\n",
        "        self.clip_length = clip_length    \n",
        "        self.transformer = Transformer(dim_embedding, 8, num_layers) #8 - number of attention heads in the transformer model\n",
        "        self.linear = nn.Linear(dim_clip, clip_length * dim_embedding)  #creats an instance of the nn.linear class which is fully connected layer that maps inputs of size dim_clip to outputs of size clip_length * dim_embedding\n",
        "        #this layer will be used to map the input clip to the embeddings that will be fed into the transformer model.\n",
        "        self.prefix_const = nn.Parameter(torch.randn(prefix_length, dim_embedding), requires_grad=True) #creates a learnable paramter called prefix_const. the values of this tensor are initialized randomly using a normal distribution with mean 0 and variance 1.\n",
        "        #requires_grad = true indicating that the values will be updated during training.\n",
        "        #It will be added to the output of the transformer model to generate the final output clip.\n"
      ],
      "metadata": {
        "id": "WnaIhFLWQTkP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}